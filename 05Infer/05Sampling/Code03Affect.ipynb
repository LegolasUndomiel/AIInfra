{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b423d7e4",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 03:多模态输出采样与控制\n",
    "\n",
    "> 多模态生成模型正在改变我们创造和理解内容的方式，而采样策略则是控制这一过程的隐形艺术家。\n",
    "\n",
    "在当前的人工智能浪潮中，多模态生成模型已经成为内容创作的重要工具。无论是通过文字描述生成图像，还是根据静态图像创建视频，这些模型都展现出了惊人的能力。\n",
    "\n",
    "但你是否曾经好奇，为什么同样的输入提示，有时会产生令人惊艳的结果，有时却平平无奇？这背后往往取决于采样策略的选择。\n",
    "\n",
    "今天我们将深入探讨温度（Temperature）和 Top-P（核采样）两种采样策略如何影响多模态生成模型的输出结果，以及如何通过调整这些参数来平衡生成结果的**多样性**、**创造性**和**质量**。\n",
    "\n",
    "### 1. Temperature 采样\n",
    "\n",
    "温度参数或许是控制生成随机性最直观的方式。它在数学上调整了模型输出概率分布的平滑程度。\n",
    "\n",
    "给定原始概率分布 $P(x_i|x_{<i})$，应用温度参数 $T$ 后的新概率分布为：\n",
    "\n",
    "$$\\hat{P}(x_i|x_{<i}) = \\frac{\\exp(\\frac{z_i}{T})}{\\sum_j \\exp(\\frac{z_j}{T})}$$\n",
    "\n",
    "其中 $z_i$ 是模型输出的 logits 值。\n",
    "\n",
    "**温度参数的影响**：\n",
    "\n",
    "- 当 $T > 1$ 时，概率分布变得更加平滑，生成结果更加多样但可能不够准确\n",
    "- 当 $T < 1$ 时，概率分布更加尖锐，生成结果更加确定但可能缺乏创造性\n",
    "\n",
    "为了更好地理解这一概念，让我们通过代码来实现温度采样的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820f211e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始概率: [0.56937605 0.20946175 0.12704498 0.09411723]\n",
      "温度 0.5 后的概率: [0.8247789  0.11162169 0.04106332 0.02253603]\n",
      "温度 1.0 后的概率: [0.56937605 0.20946175 0.12704498 0.09411723]\n",
      "温度 2.0 后的概率: [0.4023389  0.24403088 0.19005144 0.1635788 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def apply_temperature(logits, temperature):\n",
    "    if temperature > 0:\n",
    "        # 应用温度缩放\n",
    "        scaled_logits = logits / temperature\n",
    "        # 重新计算 softmax 概率\n",
    "        probs = F.softmax(scaled_logits, dim=-1)\n",
    "        return probs\n",
    "    else:\n",
    "        raise ValueError(\"温度参数必须大于 0\")\n",
    "\n",
    "# 示例用法\n",
    "logits = torch.tensor([2.0, 1.0, 0.5, 0.2])\n",
    "print(\"原始概率:\", F.softmax(logits, dim=-1).numpy())\n",
    "\n",
    "for temp in [0.5, 1.0, 2.0]:\n",
    "    temp_probs = apply_temperature(logits, temp)\n",
    "    print(f\"温度 {temp} 后的概率: {temp_probs.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c49796",
   "metadata": {},
   "source": [
    "这段代码展示了温度参数如何影响概率分布。当温度值较低时（如 0.5），概率分布更加集中，模型更倾向于选择最高概率的选项；而当温度值较高时（如 2.0），概率分布更加平滑，模型的选择更加多样化。\n",
    "\n",
    "## 2. Top-P 采样\n",
    "\n",
    "Top-P 采样，也称为核采样，选择概率累积超过阈值 p 的最小可能词元集合，然后从这个集合中重新归一化概率并采样。\n",
    "\n",
    "形式上，给定概率分布 $P$ 和阈值 $p ∈ (0, 1]$，我们按概率降序排列，找到最小的集合 $S$ 使得：\n",
    "\n",
    "$$\\sum_{x_i \\in S} P(x_i) \\geq p$$\n",
    "\n",
    "然后从集合 $S$ 中按照重新归一化的概率进行采样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab3fcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始概率: [0.4 0.3 0.2 0.1]\n",
      "Top-P (p=0.7) 后的概率: [0.57142854 0.4285714  0.         0.        ]\n",
      "Top-P (p=0.9) 后的概率: [0.44444445 0.33333334 0.22222222 0.        ]\n"
     ]
    }
   ],
   "source": [
    "def top_p_sampling(probs, p):\n",
    "    # 对概率进行排序\n",
    "    sorted_probs, indices = torch.sort(probs, descending=True)\n",
    "    # 计算累积概率\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    \n",
    "    # 移除累积概率超过 p 的部分\n",
    "    indices_to_remove = cumulative_probs > p\n",
    "    # 确保至少保留一个 token\n",
    "    indices_to_remove[..., 1:] = indices_to_remove[..., :-1].clone()\n",
    "    indices_to_remove[..., 0] = 0\n",
    "    \n",
    "    # 将需要移除的 token 概率设为 0\n",
    "    sorted_probs[indices_to_remove] = 0\n",
    "    # 重新归一化概率\n",
    "    sorted_probs /= sorted_probs.sum()\n",
    "    \n",
    "    # 恢复到原始顺序\n",
    "    return sorted_probs.scatter(-1, indices, sorted_probs)\n",
    "\n",
    "# 示例用法\n",
    "probs = torch.tensor([0.4, 0.3, 0.2, 0.1])\n",
    "print(\"原始概率:\", probs.numpy())\n",
    "\n",
    "for p in [0.7, 0.9]:\n",
    "    top_p_probs = top_p_sampling(probs, p)\n",
    "    print(f\"Top-P (p={p}) 后的概率: {top_p_probs.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3157d",
   "metadata": {},
   "source": [
    "Top-P 采样的优势在于它能够动态调整候选集的大小，既保证了多样性，又避免了选择概率极低的选项。\n",
    "\n",
    "## 3. 文生图应用实践\n",
    "\n",
    "了解了采样策略的基本原理后，让我们看看它们在实际的多模态生成任务中如何应用。我们将使用 Stable Diffusion 模型进行文本到图像的生成实验。\n",
    "\n",
    "首先，我们需要设置环境并加载模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0138e93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yswang/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 19 files: 100%|██████████| 19/19 [15:45<00:00, 49.76s/it]\n",
      "Loading pipeline components...:  43%|████▎     | 3/7 [00:01<00:01,  2.22it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:02<00:00,  3.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionXLPipeline\n",
    "import torch\n",
    "\n",
    "# 加载 SDXL 管道\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", \n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "\n",
    "# 将管道移动到 GPU（如果可用）\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79d495",
   "metadata": {},
   "source": [
    "虽然 Stable Diffusion 本身不直接暴露温度参数，但我们可以通过修改生成过程中的随机性来模拟类似效果。实际上，在扩散模型中，类似的随机性控制可以通过调整 guidance_scale 和随机种子来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc2ca4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.66it/s]\n",
      "100%|██████████| 30/30 [00:11<00:00,  2.65it/s]\n",
      "100%|██████████| 30/30 [00:11<00:00,  2.65it/s]\n",
      "100%|██████████| 30/30 [00:11<00:00,  2.65it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_images_with_variation(prompt, num_images=4, guidance_scale=7.5):\n",
    "    images = []\n",
    "    for i in range(num_images):\n",
    "        # 使用不同的随机种子\n",
    "        generator = torch.Generator(device=device).manual_seed(i)\n",
    "        \n",
    "        # 生成图像\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=30,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=generator,\n",
    "        ).images[0]\n",
    "        \n",
    "        images.append(image)\n",
    "    \n",
    "    return images\n",
    "\n",
    "# 设置生成参数\n",
    "prompt = \"一个美丽的日落海滩，有椰子树和金色的沙滩\"\n",
    "negative_prompt = \"模糊，失真，低质量\"\n",
    "\n",
    "# 生成图像\n",
    "images = generate_images_with_variation(prompt)\n",
    "# 保存图像\n",
    "for i, img in enumerate(images):\n",
    "    img.save(f\"./images/CODE03generated_{i}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eacac3c",
   "metadata": {},
   "source": [
    "在实际应用中，我们往往需要更精细的控制，而不仅仅是调整随机种子。多模态生成的可控性技术正在不断发展，例如通过 ControlNet 实现空间精准定位，通过 LoRA 注入特定规则，以及通过 CLIP 进行情感校准等方法。\n",
    "\n",
    "## 4. 多模态实验\n",
    "\n",
    "除了文生图应用，采样策略也对多模态语言模型的输出有重要影响。让我们以视觉语言模型为例，看看不同采样参数如何影响模型生成的描述。\n",
    "\n",
    "我们将使用 Qwen-VL 模型进行图文对话实验，观察不同温度和 Top-P 参数如何影响生成的图像描述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ccc46e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-15 15:42:03 [utils.py:326] non-default args: {'model': 'Qwen/Qwen2.5-VL-7B-Instruct', 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'limit_mm_per_prompt': {'image': 10, 'video': 10}}\n",
      "INFO 11-15 15:42:13 [__init__.py:711] Resolved architecture: Qwen2_5_VLForConditionalGeneration\n",
      "INFO 11-15 15:42:13 [__init__.py:1750] Using max model len 128000\n",
      "INFO 11-15 15:42:13 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-15 15:42:18 [__init__.py:241] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m INFO 11-15 15:42:19 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m INFO 11-15 15:42:19 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen2.5-VL-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-VL-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-VL-7B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 691, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 492, in __init__\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 80, in __init__\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 48, in _init_executor\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]     self.collective_rpc(\"init_device\")\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/utils/__init__.py\", line 3007, in run_method\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 603, in init_device\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 179, in init_device\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700]     raise ValueError(\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ERROR 11-15 15:42:21 [core.py:700] ValueError: Free memory on device (9.7/47.53 GiB) on startup is less than desired GPU memory utilization (0.6, 28.52 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m Process EngineCore_0:\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m   File \"/home/yswang/miniforge3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m   File \"/home/yswang/miniforge3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 704, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 691, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 492, in __init__\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 80, in __init__\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 48, in _init_executor\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m     self.collective_rpc(\"init_device\")\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/utils/__init__.py\", line 3007, in run_method\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 603, in init_device\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m   File \"/home/yswang/miniforge3/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 179, in init_device\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m     raise ValueError(\n",
      "\u001b[1;36m(EngineCore_0 pid=68579)\u001b[0;0m ValueError: Free memory on device (9.7/47.53 GiB) on startup is less than desired GPU memory utilization (0.6, 28.52 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m processor = AutoProcessor.from_pretrained(MODEL_PATH)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 初始化 LLM\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimit_mm_per_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvideo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_descriptions_with_sampling\u001b[39m(image_path, prompt_text, sampling_configs):\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# 构建消息\u001b[39;00m\n\u001b[32m     17\u001b[39m     messages = [\n\u001b[32m     18\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant.\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     19\u001b[39m         {\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m         },\n\u001b[32m     26\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/vllm/entrypoints/llm.py:285\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    282\u001b[39m log_non_default_args(engine_args)\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    289\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/vllm/engine/llm_engine.py:490\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    488\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:127\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    126\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:104\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28mself\u001b[39m.output_processor = OutputProcessor(\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m    101\u001b[39m                                         log_stats=\u001b[38;5;28mself\u001b[39m.log_stats)\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[38;5;28mself\u001b[39m.engine_core.engine_core.model_executor  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:80\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient.make_async_mp_client(\n\u001b[32m     77\u001b[39m         vllm_config, executor_class, log_stats)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:600\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    599\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28mself\u001b[39m.is_dp = \u001b[38;5;28mself\u001b[39m.vllm_config.parallel_config.data_parallel_size > \u001b[32m1\u001b[39m\n\u001b[32m    608\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:446\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[39m\n\u001b[32m    442\u001b[39m     \u001b[38;5;28mself\u001b[39m.stats_update_address = client_addresses.get(\n\u001b[32m    443\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstats_update_address\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    445\u001b[39m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlaunch_core_engines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_manager\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/contextlib.py:144\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/vllm/v1/engine/utils.py:706\u001b[39m, in \u001b[36mlaunch_core_engines\u001b[39m\u001b[34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/vllm/v1/engine/utils.py:759\u001b[39m, in \u001b[36mwait_for_engine_startup\u001b[39m\u001b[34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[39m\n\u001b[32m    757\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process.exitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    758\u001b[39m         finished[coord_process.name] = coord_process.exitcode\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    760\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    761\u001b[39m                        \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    763\u001b[39m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[32m    764\u001b[39m eng_identity, ready_msg_bytes = handshake_socket.recv_multipart()\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# 初始化模型和处理器\n",
    "MODEL_PATH = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# 初始化 LLM\n",
    "llm = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    limit_mm_per_prompt={\"image\": 10, \"video\": 10},\n",
    "    gpu_memory_utilization=0.6,\n",
    ")\n",
    "\n",
    "def generate_descriptions_with_sampling(image_path, prompt_text, sampling_configs):\n",
    "    # 构建消息\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path},\n",
    "                {\"type\": \"text\", \"text\": prompt_text},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # 应用聊天模板\n",
    "    prompt = processor.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    descriptions = []\n",
    "    for config in sampling_configs:\n",
    "        # 设置采样参数\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=config[\"temperature\"],\n",
    "            top_p=config[\"top_p\"],\n",
    "            max_tokens=256,\n",
    "        )\n",
    "        \n",
    "        # 生成描述\n",
    "        outputs = llm.generate([{\"prompt\": prompt}], sampling_params=sampling_params)\n",
    "        description = outputs[0].outputs[0].text.strip()\n",
    "        descriptions.append(description)\n",
    "    \n",
    "    return descriptions\n",
    "\n",
    "# 定义不同的采样配置\n",
    "sampling_configs = [\n",
    "    {\"temperature\": 0.1, \"top_p\": 0.9, \"name\": \"低温度，高 Top-P\"},\n",
    "    {\"temperature\": 0.7, \"top_p\": 0.9, \"name\": \"中等温度，高 Top-P\"},\n",
    "    {\"temperature\": 1.2, \"top_p\": 0.9, \"name\": \"高温度，高 Top-P\"},\n",
    "    {\"temperature\": 0.7, \"top_p\": 0.3, \"name\": \"中等温度，低 Top-P\"},\n",
    "]\n",
    "\n",
    "# 生成描述（需要实际图像路径）\n",
    "image_path = \"path_to_your_image.jpg\"\n",
    "prompt_text = \"描述这张图片中的场景和细节\"\n",
    "descriptions = generate_descriptions_with_sampling(image_path, prompt_text, sampling_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da343b",
   "metadata": {},
   "source": [
    "通过这个实验，我们可以观察到不同采样配置下模型生成的描述有何不同。低温度配置往往产生更加保守和确定的描述，而高温度配置则可能产生更加创造性和多样化的描述，但也可能增加不相关或虚构内容的风险。\n",
    "\n",
    "## 5. 评估生成结果\n",
    "\n",
    "评估多模态生成结果的质量是一个复杂的任务，需要从多个维度进行考量。常用的评估指标包括：\n",
    "\n",
    "1.  **模态对齐度（MDA）**：衡量生成内容与输入提示之间的一致性。\n",
    "2.  **细节保真度（DF）**：评估生成内容的细节丰富程度和准确性。\n",
    "3.  **多样性**：衡量不同生成结果之间的差异程度。\n",
    "4.  **创造性**：评估生成内容的新颖性和创新程度。\n",
    "\n",
    "我们可以通过计算一些定量指标来评估生成结果的多样性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def calculate_diversity(descriptions):\n",
    "    \"\"\"\n",
    "    计算生成描述的多样性指标\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    for desc in descriptions:\n",
    "        words = desc.lower().split()\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    # 计算词汇总量和唯一词汇量\n",
    "    total_words = len(all_words)\n",
    "    unique_words = len(set(all_words))\n",
    "    lexical_diversity = unique_words / total_words if total_words > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"lexical_diversity\": lexical_diversity,\n",
    "        \"unique_words\": unique_words,\n",
    "        \"total_words\": total_words\n",
    "    }\n",
    "\n",
    "# 计算生成描述的多样性\n",
    "diversity_metrics = calculate_diversity(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f844d01",
   "metadata": {},
   "source": [
    "除了这些定量指标，人工评估仍然是评估生成质量的重要方式，特别是对于创造性和审美价值的判断。\n",
    "\n",
    "## 6. 总结与思考\n",
    "\n",
    "采样策略在多模态生成中扮演着至关重要的角色，它们像是隐形的艺术家，默默地影响着生成结果的多样性、创造性和质量。通过理解和掌握温度参数和 Top-P 采样等策略，我们能够更好地驾驭多模态生成模型，创造出更加符合期望的内容。\n",
    "\n",
    "需要注意的是，参数调整并非万能，它需要在模型能力、任务需求和使用场景之间找到平衡点。有时候，**创造力的提升可能会以降低精确性为代价**，而**过于追求确定性又可能抑制创新**。这正是多模态生成既是一门科学也是一门艺术的原因。\n",
    "\n",
    "希望本文能够为你理解和应用采样策略提供有益的指导，帮助你在多模态生成的探索之旅中走得更远。记住，最好的参数配置往往来自于不断的实验和调整，而不是一成不变的公式。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
